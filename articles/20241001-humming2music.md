---
title: "[論文] 鼻歌から楽曲を生成！Humming2Musicの論文要約"
emoji: "🎧"
type: "idea"
topics: [論文, 音楽]
published: true
---

# はじめに
本記事では，IJCAI 2023で発表された論文"Humming2Music: Being A Composer As Long As You Can Humming"の内容をまとめる．この論文では，鼻歌を入力として受け取り，それを元に完全な楽曲を自動生成するシステムを提案している．システムは5つのモジュールから構成されており，ユーザによって入力された鼻歌を楽譜に変換し，メロディを生成，伴奏を加え，最終的に音声合成を行う．提案システムは，音楽の専門知識がないユーザでも簡単に楽曲を作成できることを目指している．

https://www.ijcai.org/proceedings/2023/840

# Introduction
近年，音楽生成技術は急速に進歩しており，多くの手法が提案されいている．ユーザの入力に基づいてカスタマイズされた楽曲を生成する技術も提案されているが，歌詞やコード進行に基づくものが多く，よりユーザが直感的に入力可能な鼻歌に基づく生成技術は未だ少ない．さらに，既存の鼻歌による入力をサポートする手法は，鼻歌から伴奏の生成のみを行うため，ユーザが完全なメロディを鼻歌で入力する必要がありハードルが高い．

![全体構成](/images/20241001-humming2music/overview.png)

そこで，この論文では鼻歌を入力として受け取り，それを元に完全な楽曲を自動生成するシステムを提案している．このシステムは，音楽知識がないユーザでも簡単に楽曲を作成できることを目指している．システムは上の図のような構成になっており，5つのモジュールから構成されている．

# Methodology
Methotologyでは，提案システムにおける各モジュールの詳細な説明が行われていた．

## 1. 鼻歌の楽譜変換 (Humming Transcription)
最初にユーザーが入力した鼻歌をMIDI形式の楽譜に変換する．既存の音楽トランスクリプション手法の多くはフレームベースであり，音符のオンセットやオフセット，音程を抽出する2段階の方法を採用している．しかし，人間の声は楽器音とは異なり，音のオンセットが不明瞭であるため，このアプローチでは正確な変換が困難である．

この問題を解決するために，音響信号をスペクトログラムに変換し，それをYOLOXモデルに入力して音符の位置と範囲を検出する**MusicYOLO**という手法を採用している．具体的には，鼻歌の音声をまずCQTスペクトログラムに変換し，それを3チャンネルの画像に変換する．この画像がYOLOXモデルに入力され，音符の位置を示すバウンディングボックスを出力する．これにより，音符のオンセットやオフセットが検出され，最終的に音符情報がMIDI形式に変換される．さらに，鼻歌から生成された不協和音を微調整するアルゴリズムも組み込まれており，これによって次のメロディ生成の精度が向上する．

## 2. メロディ生成 (Melody Generation)
楽譜変換が完了すると，そのデータを元に，完全なメロディを生成する．このモジュールでは，鼻歌のメロディに基づき，コード進行やリズムテンプレートを適用して，自然なメロディを補完する．具体的には，鼻歌から抽出された音符情報に対して，コードとリズムのテンプレートライブラリから最適なテンプレートを選択し，メロディを生成している．コードテンプレートの選択は，鼻歌の最初の小節の音階とコードテンプレートの音階を比較し，最も一致するものを選択している．同様に，リズムテンプレートは鼻歌のオンセットとテンプレートのオンセットを比較し選択される．

このプロセスにより，テンプレートライブラリの中から最適なコードおよびリズムのテンプレートが選ばれ，鼻歌を元にしたメロディが完成する．

## 3. 分散和音生成 (Broken Chord Generation)
次に，生成されたメロディに分散和音を加えることで，楽曲の表現力と豊かさを向上させる．このモジュールは**Transformer**に基づいたモデルを使用しており，メロディとコードテンプレートを入力として，対応する分散和音を生成する．

分散和音生成モデルでは，各音符の位置，ピッチ，持続時間をトークンとして扱い，それらをメロディに適合させるように生成する．特に，メロディと和音の整合性を保つために，時間やピッチに基づいた整列規則がモデルに導入されている．これにより，分散和音が自然にメロディと一致し，楽曲全体が調和するようになる．

## 4. 伴奏生成 (Accompaniment Generation)
伴奏生成モジュールでは，ベースやストリングス，ドラムなどの追加トラックを生成し，楽曲全体を豊かにする．伴奏生成は，音楽理論に基づくルールベースのアプローチを使用している．具体的には，ドラムパターンはライブラリからランダムに選択され，ベースやストリングスはコード進行に基づいて調整される．各パターンはC調で作成されており，選択されたパターンのルート音がコードに応じて適切に変更される．

## 5. 音声合成 (Audio Synthesis)
最終段階では，各トラックをミキシングし，適切な音量バランスを調整する．また，リバーブやコンプレッサといったエフェクトを加え，最終的な楽曲を完成させる．これらの処理は，Pythonパッケージ**Pedalboard**を用いて実装されている．特に，コンプレッサは音割れを防ぎ，音声の安定性や明瞭さを保つために重要な役割を果たしている．

これらの一連のプロセスを通じて，ユーザーが入力した鼻歌を元に，高品質で表現力豊かな楽曲が自動的に生成される．

# Results and Conclusion
対人評価によって提案システムは，ユーザーの鼻歌を入力として高品質な音楽を自動生成できることが確認された．具体的な成果は以下の通りである．

- 正確なトランスクリプション
  ユーザーの鼻歌は正確にMIDI形式に変換され，高精度な楽譜が生成される．
- 高品質なメロディ生成
  メロディの長さが増しても，テンプレートに基づいて一貫した高品質なメロディが生成される．
- 鼻歌の反映
  鼻歌の特徴が最終的な楽曲にしっかりと反映されており，自然でユーザーの意図を反映した楽曲が生成される．
- 独自性のある楽曲生成
  使用されるテンプレートはコードとリズム情報のみであり，最終的な音楽はオリジナルの楽曲とは大きく異なる独自のものとなる．
- ユーザーの満足度
  約46%のユーザーが生成された音楽を「悪くない」と評価しており，全体的に肯定的なフィードバックが得られている．

まとめると，このシステムは音楽の専門知識がないユーザーでも，直感的に鼻歌を使って楽曲を作成できる有用なツールとして評価されている．